{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47b2a9a2-4223-4001-a0ac-48e9a722fb43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**DataFrames**\n",
    "\n",
    "### Understanding DataFrames\n",
    "\n",
    "**1. What is a Schema?**\n",
    "A schema defines the structure of a dataset, specifying the column names, data types, and constraints. It ensures that the data adheres to a predefined format, enabling consistency and compatibility across processing workflows.\n",
    "\n",
    "- **Benefits of Schema:**\n",
    "  - Enforces data integrity and validation.\n",
    "  - Enables query optimization.\n",
    "  - Facilitates compatibility with downstream applications.\n",
    "\n",
    "*Example*: Viewing the schema of a DataFrame:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "829d4a8e-4f73-47c8-abaa-c0ec3027feae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**2. DataFrames**\n",
    "A DataFrame is a distributed collection of data organized into named columns, similar to a table in a relational database. It is designed to support both structured and semi-structured data processing.\n",
    "\n",
    "- **Key Features of DataFrames:**\n",
    "  - Schema-based processing.\n",
    "  - Lazy evaluation for efficient execution.\n",
    "  - Catalyst optimizer for query planning.\n",
    "  - Multi-language support: Python, Scala, SQL, R.\n",
    "\n",
    "*Example*: Loading a JSON dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87fb8d94-2144-4cf8-8cd6-cf1013f3aef3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load JSON data\n",
    "flight_data_json = \"dbfs:/mnt/data/data/flight-data/json/2015-summary.json\"\n",
    "df = spark.read.format(\"json\").load(flight_data_json)\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8dc8b9b4-a93d-47f9-87b6-fafdbff14b41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a19cf9e6-db61-4c6b-be32-14f1c5e1dced",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### File Formats in Spark\n",
    "\n",
    "**1. JSON (JavaScript Object Notation)**\n",
    "A lightweight, semi-structured format ideal for data exchange.\n",
    "\n",
    "- **Advantages:**\n",
    "  - Human-readable.\n",
    "  - Flexible structure for nested data.\n",
    "\n",
    "*Example*: Reading and writing JSON:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "486dbc32-52cc-4936-96d8-f9f501f2b6e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reading JSON\n",
    "json_df = spark.read.format(\"json\").load(\"dbfs:/mnt/data/data/flight-data/json/2015-summary.json\")\n",
    "\n",
    "# Writing JSON\n",
    "json_df.write.format(\"json\").save(\"dbfs:/mnt/data/output/flight-data-json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ee8349b-d4bf-4b63-b8ea-da86f111d17b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**2. Parquet**\n",
    "A columnar storage format optimized for analytics, offering efficient compression and query performance.\n",
    "\n",
    "- **Advantages:**\n",
    "  - Supports schema evolution.\n",
    "  - Optimized for columnar operations.\n",
    "\n",
    "*Example*: Working with Parquet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4943ed9b-0d4a-4266-ac84-77201a6d27e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reading Parquet\n",
    "parquet_df = spark.read.format(\"parquet\").load(\"dbfs:/mnt/data/data/flight-data/parquet/2010-summary.parquet\")\n",
    "\n",
    "# Writing Parquet\n",
    "parquet_df.write.format(\"parquet\").save(\"dbfs:/mnt/data/output/flight-data-parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c1f1705-c7ee-4464-99c1-2bcdd225c2eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**3. Avro**\n",
    "A row-based binary format commonly used for data serialization.\n",
    "\n",
    "- **Advantages:**\n",
    "  - Compact and efficient.\n",
    "  - Supports schema definition and evolution.\n",
    "\n",
    "*Example*: Working with Avro:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "007b8ffd-7c50-4e25-9f56-3269444cc6d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "parquet_df.write.format(\"avro\").save(\"dbfs:/mnt/data/output/flight-data-avro\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7515e31d-1d4c-45fd-98f9-d13b36ee4b91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reading Avro\n",
    "avro_df = spark.read.format(\"avro\").load(\"dbfs:/mnt/data/output/flight-data-avro\")\n",
    "avro_df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "515ed97e-e696-47fc-b313-b5138d729cbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**4. CSV (Comma-Separated Values)**\n",
    "A widely used format for simple tabular data.\n",
    "\n",
    "- **Advantages:**\n",
    "  - Easy to understand and use.\n",
    "  - Compatible with many tools.\n",
    "\n",
    "*Example*: Working with CSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82a54021-6cdd-4b8b-ab1b-7415ddfb89c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reading CSV\n",
    "csv_df = spark.read.format(\"csv\").option(\"header\", True).load(\"dbfs:/mnt/data/data/flight-data/csv/2010-summary.csv\")\n",
    "\n",
    "# Writing CSV\n",
    "csv_df.write.format(\"csv\").option(\"header\", True).save(\"dbfs:/mnt/data/output/flight-data-csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9393babb-d73c-4a0e-b456-31e5b5e1e456",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "### Schema Management and Optimization\n",
    "\n",
    "**1. Inferring Schema**\n",
    "Spark automatically detects column names and data types based on file contents.\n",
    "\n",
    "*Example*: Inferring schema from JSON:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f9f2dcb-22cf-4d36-be63-81cb9aeea21b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schema_inferred_df = spark.read.json(flight_data_json)\n",
    "schema_inferred_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1505042e-1223-4949-89fd-309c8d47d9d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**2. Explicit Schema Definition**\n",
    "Define schemas for strict data validation and compatibility.\n",
    "\n",
    "*Example*: Defining a schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e14fc9ef-d010-44c1-b781-046ae704445b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, LongType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"DEST_COUNTRY_NAME\", StringType(), True),\n",
    "    StructField(\"ORIGIN_COUNTRY_NAME\", StringType(), True),\n",
    "    StructField(\"count\", LongType(), True)\n",
    "])\n",
    "\n",
    "schema_defined_df = spark.read.format(\"json\").schema(schema).load(flight_data_json)\n",
    "schema_defined_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7182897-4096-4d26-9c73-724930e10bbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2bbdfa93-aab6-4512-9b9c-7b36f8e7da93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "M3-DataFrames-and-Datasets",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
