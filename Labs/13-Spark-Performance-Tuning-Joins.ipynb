{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d97ac5f6-cbfc-4a5a-8137-d65773c31e9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+----------+\n",
      "|     city| country|population|\n",
      "+---------+--------+----------+\n",
      "| medellin|colombia|       2.5|\n",
      "|bangalore|   india|      12.3|\n",
      "+---------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "citiesDF = (\n",
    "  (\"medellin\", \"colombia\", 2.5),\n",
    "  (\"bangalore\", \"india\", 12.3)\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "citiesDF_a = sc.parallelize(citiesDF)\n",
    "citiesDF_b = spark.createDataFrame(citiesDF_a, (\"city\", \"country\", \"population\"))\n",
    "\n",
    "citiesDF_b.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6398792b-2c4c-4334-875b-c132ac8fc01c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "Join Inner, (city#4454 = city#4466)\n",
      ":- LogicalRDD [first_name#4453, city#4454], false\n",
      "+- ResolvedHint (strategy=broadcast)\n",
      "   +- LogicalRDD [city#4466, country#4467, population#4468], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "first_name: string, city: string, city: string, country: string, population: double\n",
      "Join Inner, (city#4454 = city#4466)\n",
      ":- LogicalRDD [first_name#4453, city#4454], false\n",
      "+- ResolvedHint (strategy=broadcast)\n",
      "   +- LogicalRDD [city#4466, country#4467, population#4468], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Join Inner, (city#4454 = city#4466), rightHint=(strategy=broadcast), joinId=5\n",
      ":- Filter isnotnull(city#4454)\n",
      ":  +- LogicalRDD [first_name#4453, city#4454], false\n",
      "+- Filter isnotnull(city#4466)\n",
      "   +- LogicalRDD [city#4466, country#4467, population#4468], false\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- == Initial Plan ==\n",
      "   BroadcastHashJoin [city#4454], [city#4466], Inner, BuildRight, false, true\n",
      "   :- Filter isnotnull(city#4454)\n",
      "   :  +- Scan ExistingRDD[first_name#4453,city#4454]\n",
      "   +- Exchange SinglePartition, EXECUTOR_BROADCAST, [plan_id=2001]\n",
      "      +- Filter isnotnull(city#4466)\n",
      "         +- Scan ExistingRDD[city#4466,country#4467,population#4468]\n",
      "\n",
      "== Photon Explanation ==\n",
      "Photon does not fully support the query because:\n",
      "\t\tUnsupported node: Scan ExistingRDD[first_name#4453,city#4454].\n",
      "\n",
      "Reference node:\n",
      "\tScan ExistingRDD[first_name#4453,city#4454]\n",
      "\n",
      "Photon does not fully support the query because:\n",
      "\t\tUnsupported node: Scan ExistingRDD[city#4466,country#4467,population#4468].\n",
      "\n",
      "Reference node:\n",
      "\tScan ExistingRDD[city#4466,country#4467,population#4468]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleDF_b_j = peopleDF_b.join(\n",
    "  broadcast(citiesDF_b),\n",
    "  peopleDF_b.city == citiesDF_b.city\n",
    ")\n",
    "\n",
    "peopleDF_b_j.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b20e3f3f-bbb3-474c-96b2-00fc148ea6a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "Join Inner, (city#4454 = city#4466)\n",
      ":- LogicalRDD [first_name#4453, city#4454], false\n",
      "+- LogicalRDD [city#4466, country#4467, population#4468], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "first_name: string, city: string, city: string, country: string, population: double\n",
      "Join Inner, (city#4454 = city#4466)\n",
      ":- LogicalRDD [first_name#4453, city#4454], false\n",
      "+- LogicalRDD [city#4466, country#4467, population#4468], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Join Inner, (city#4454 = city#4466)\n",
      ":- Filter isnotnull(city#4454)\n",
      ":  +- LogicalRDD [first_name#4453, city#4454], false\n",
      "+- Filter isnotnull(city#4466)\n",
      "   +- LogicalRDD [city#4466, country#4467, population#4468], false\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- == Initial Plan ==\n",
      "   SortMergeJoin [city#4454], [city#4466], Inner\n",
      "   :- Sort [city#4454 ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(city#4454, 200), ENSURE_REQUIREMENTS, [plan_id=2055]\n",
      "   :     +- Filter isnotnull(city#4454)\n",
      "   :        +- Scan ExistingRDD[first_name#4453,city#4454]\n",
      "   +- Sort [city#4466 ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(city#4466, 200), ENSURE_REQUIREMENTS, [plan_id=2056]\n",
      "         +- Filter isnotnull(city#4466)\n",
      "            +- Scan ExistingRDD[city#4466,country#4467,population#4468]\n",
      "\n",
      "== Photon Explanation ==\n",
      "Photon does not fully support the query because:\n",
      "\t\tUnsupported node: Scan ExistingRDD[first_name#4453,city#4454].\n",
      "\n",
      "Reference node:\n",
      "\tScan ExistingRDD[first_name#4453,city#4454]\n",
      "\n",
      "Photon does not fully support the query because:\n",
      "\t\tUnsupported node: Scan ExistingRDD[city#4466,country#4467,population#4468].\n",
      "\n",
      "Reference node:\n",
      "\tScan ExistingRDD[city#4466,country#4467,population#4468]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleDF_b_j = peopleDF_b.join(\n",
    "  citiesDF_b,\n",
    "  peopleDF_b.city == citiesDF_b.city\n",
    ")\n",
    "\n",
    "peopleDF_b_j.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf48aa19-ee14-4ea3-ad0d-52a5aeabfd1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Notice how the parsed, analyzed, and optimized logical plans all contain ResolvedHint isBroadcastable=true because the broadcast() function was used. This hint isn’t included when the broadcast() function isn’t used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4a09476-4f72-4b66-8352-33d86d9c9118",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Automatic Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49f8f52b-0391-41a5-920f-d8e3d5d08fa3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- In many cases, Spark can automatically detect whether to use a broadcast join or not, depending on the size of the data. If Spark can detect that one of the joined DataFrames is small (10 MB by default), Spark will automatically broadcast it for us. The code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1798534-c5d3-4c4b-a663-90acb11844d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Join UsingJoin(Inner, [id])\n",
      ":- Range (1, 10000, step=1, splits=Some(4))\n",
      "+- Range (1, 100000000, step=1, splits=Some(4))\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: bigint\n",
      "Project [id#4511L]\n",
      "+- Join Inner, (id#4511L = id#4509L)\n",
      "   :- Range (1, 10000, step=1, splits=Some(4))\n",
      "   +- Range (1, 100000000, step=1, splits=Some(4))\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [id#4511L]\n",
      "+- Join Inner, (id#4511L = id#4509L)\n",
      "   :- Range (1, 10000, step=1, splits=Some(4))\n",
      "   +- Range (1, 100000000, step=1, splits=Some(4))\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- == Initial Plan ==\n",
      "   ColumnarToRow\n",
      "   +- PhotonResultStage\n",
      "      +- PhotonProject [id#4511L]\n",
      "         +- PhotonBroadcastHashJoin [id#4511L], [id#4509L], Inner, BuildLeft, false, true\n",
      "            :- PhotonShuffleExchangeSource\n",
      "            :  +- PhotonShuffleMapStage\n",
      "            :     +- PhotonShuffleExchangeSink SinglePartition\n",
      "            :        +- PhotonRange Range (1, 10000, step=1, splits=4)\n",
      "            +- PhotonRange Range (1, 100000000, step=1, splits=4)\n",
      "\n",
      "== Photon Explanation ==\n",
      "The query is fully supported by Photon.\n"
     ]
    }
   ],
   "source": [
    "bigTable = spark.range(1, 100000000)\n",
    "smallTable = spark.range(1, 10000) # size estimated by Spark - auto-broadcast\n",
    "joinedNumbers = smallTable.join(bigTable, \"id\")\n",
    "joinedNumbers.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f1b1381-bce6-4916-ab1d-3ca6c866a3ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- However, in the previous case, Spark did not detect that the small table could be broadcast. \n",
    "- The reason is that Spark will not determine the size of a local collection because it might be big, and evaluating its size may be an O(N) operation, which can defeat the purpose before any computation is made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b5526bd-9932-437f-b5e2-ce98f093a862",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Spark will perform auto-detection when\n",
    "    - it constructs a DataFrame from scratch, e.g. spark.range\n",
    "    - it reads from files with schema and/or size information, e.g. Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "087be9f6-396f-4b31-a918-03689d5db50e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Configuring Broadcast Join Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "475dad7e-fae3-42fa-a823-32aa629343ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- The threshold for automatic broadcast join detection can be tuned or disabled.\n",
    "- The configuration is spark.sql.autoBroadcastJoinThreshold, and the value is taken in bytes. If you want to configure it to another number, we can set it in the SparkSession:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5ad489a-f5d7-4575-bd52-f5f99fcebcf5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 104857600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eddd19e6-da01-4f71-99ec-cab2f8eef41e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- or deactivate it altogether by setting the value to -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b0ef0ca-875d-4ac8-b206-74566851c564",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d178623-119d-4e28-a239-8f3cf6d12385",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04-Spark-Performance-Tuning-Joins",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
