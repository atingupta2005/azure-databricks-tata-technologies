{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5da57f7d-98f0-41c5-a0c2-f66e49315760",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----+-----------+\n|Add|    ID|Name|partitionID|\n+---+------+----+-----------+\n|USA|21.528|Jhon|          0|\n|USA|  3.69| Joe|          1|\n|IND|  2.48|Tina|          2|\n|USA| 22.22|Jhon|          3|\n|INA|  5.33| Joe|          3|\n+---+------+----+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import spark_partition_id\n",
    "data1 = [{'Name':'Jhon','ID':21.528,'Add':'USA'},{'Name':'Joe','ID':3.69,'Add':'USA'},{'Name':'Tina','ID':2.48,'Add':'IND'},{'Name':'Jhon','ID':22.22, 'Add':'USA'},{'Name':'Joe','ID':5.33,'Add':'INA'}]\n",
    "a = sc.parallelize(data1)\n",
    "b = spark.createDataFrame(a)\n",
    "b=b.withColumn(\"partitionID\", spark_partition_id())\n",
    "b.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2397e9a0-b6ec-4200-a8aa-0e1afaab79c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----+-----------+\n|  Add|    ID|Name|partitionID|\n+-----+------+----+-----------+\n|India|21.528|Atin|          0|\n| USeA|  3.69| Joe|          1|\n|  IND|  2.48|Tina|          2|\n| USdA| 22.22|Jhon|          3|\n|  rsa|  5.33| Joe|          3|\n+-----+------+----+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "data2 = [{'Name':'Atin','ID':21.528,'Add':'India'},{'Name':'Joe','ID':3.69,'Add':'USeA'},{'Name':'Tina','ID':2.48,'Add':'IND'},{'Name':'Jhon','ID':22.22, 'Add':'USdA'},{'Name':'Joe','ID':5.33,'Add':'rsa'}]\n",
    "c = sc.parallelize(data2)\n",
    "d = spark.createDataFrame(c)\n",
    "d=d.withColumn(\"partitionID\", spark_partition_id())\n",
    "d.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a31fd6e-3f9a-42c5-ac7e-ccfbe7312e04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\nJoin Inner, (Add#4315 = Add#4287)\n:- Project [Add#4315, ID#4316, Name#4317, SPARK_PARTITION_ID() AS partitionID#4321]\n:  +- LogicalRDD [Add#4315, ID#4316, Name#4317], false\n+- Project [Add#4287, ID#4288, Name#4289, SPARK_PARTITION_ID() AS partitionID#4293]\n   +- LogicalRDD [Add#4287, ID#4288, Name#4289], false\n\n== Analyzed Logical Plan ==\nAdd: string, ID: double, Name: string, partitionID: int, Add: string, ID: double, Name: string, partitionID: int\nJoin Inner, (Add#4315 = Add#4287)\n:- Project [Add#4315, ID#4316, Name#4317, SPARK_PARTITION_ID() AS partitionID#4321]\n:  +- LogicalRDD [Add#4315, ID#4316, Name#4317], false\n+- Project [Add#4287, ID#4288, Name#4289, SPARK_PARTITION_ID() AS partitionID#4293]\n   +- LogicalRDD [Add#4287, ID#4288, Name#4289], false\n\n== Optimized Logical Plan ==\nJoin Inner, (Add#4315 = Add#4287)\n:- Filter isnotnull(Add#4315)\n:  +- Project [Add#4315, ID#4316, Name#4317, SPARK_PARTITION_ID() AS partitionID#4321]\n:     +- LogicalRDD [Add#4315, ID#4316, Name#4317], false\n+- Filter isnotnull(Add#4287)\n   +- Project [Add#4287, ID#4288, Name#4289, SPARK_PARTITION_ID() AS partitionID#4293]\n      +- LogicalRDD [Add#4287, ID#4288, Name#4289], false\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- == Initial Plan ==\n   SortMergeJoin [Add#4315], [Add#4287], Inner\n   :- Sort [Add#4315 ASC NULLS FIRST], false, 0\n   :  +- Exchange hashpartitioning(Add#4315, 200), ENSURE_REQUIREMENTS, [plan_id=1488]\n   :     +- Filter isnotnull(Add#4315)\n   :        +- Project [Add#4315, ID#4316, Name#4317, SPARK_PARTITION_ID() AS partitionID#4321]\n   :           +- Scan ExistingRDD[Add#4315,ID#4316,Name#4317]\n   +- Sort [Add#4287 ASC NULLS FIRST], false, 0\n      +- Exchange hashpartitioning(Add#4287, 200), ENSURE_REQUIREMENTS, [plan_id=1489]\n         +- Filter isnotnull(Add#4287)\n            +- Project [Add#4287, ID#4288, Name#4289, SPARK_PARTITION_ID() AS partitionID#4293]\n               +- Scan ExistingRDD[Add#4287,ID#4288,Name#4289]\n\n== Photon Explanation ==\nPhoton does not fully support the query because:\n\t\tUnsupported node: Scan ExistingRDD[Add#4315,ID#4316,Name#4317].\n\nReference node:\n\tScan ExistingRDD[Add#4315,ID#4316,Name#4317]\n\nPhoton does not fully support the query because:\n\t\tUnsupported node: Scan ExistingRDD[Add#4287,ID#4288,Name#4289].\n\nReference node:\n\tScan ExistingRDD[Add#4287,ID#4288,Name#4289]\n\n"
     ]
    }
   ],
   "source": [
    "f = d.join(b,d.Add == b.Add)\n",
    "f.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88f94d26-2b7a-43e9-b0c4-0a854419ceff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+-----------+---+----+----+-----------+\n|Add|  ID|Name|partitionID|Add|  ID|Name|partitionID|\n+---+----+----+-----------+---+----+----+-----------+\n|IND|2.48|Tina|          2|IND|2.48|Tina|          2|\n+---+----+----+-----------+---+----+----+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "f.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "531dc58e-61ea-454c-a79f-b028f1455f94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\nJoin Inner, (Add#4315 = Add#4287)\n:- Project [Add#4315, ID#4316, Name#4317, SPARK_PARTITION_ID() AS partitionID#4321]\n:  +- LogicalRDD [Add#4315, ID#4316, Name#4317], false\n+- ResolvedHint (strategy=broadcast)\n   +- Project [Add#4287, ID#4288, Name#4289, SPARK_PARTITION_ID() AS partitionID#4293]\n      +- LogicalRDD [Add#4287, ID#4288, Name#4289], false\n\n== Analyzed Logical Plan ==\nAdd: string, ID: double, Name: string, partitionID: int, Add: string, ID: double, Name: string, partitionID: int\nJoin Inner, (Add#4315 = Add#4287)\n:- Project [Add#4315, ID#4316, Name#4317, SPARK_PARTITION_ID() AS partitionID#4321]\n:  +- LogicalRDD [Add#4315, ID#4316, Name#4317], false\n+- ResolvedHint (strategy=broadcast)\n   +- Project [Add#4287, ID#4288, Name#4289, SPARK_PARTITION_ID() AS partitionID#4293]\n      +- LogicalRDD [Add#4287, ID#4288, Name#4289], false\n\n== Optimized Logical Plan ==\nJoin Inner, (Add#4315 = Add#4287), rightHint=(strategy=broadcast), joinId=3\n:- Filter isnotnull(Add#4315)\n:  +- Project [Add#4315, ID#4316, Name#4317, SPARK_PARTITION_ID() AS partitionID#4321]\n:     +- LogicalRDD [Add#4315, ID#4316, Name#4317], false\n+- Filter isnotnull(Add#4287)\n   +- Project [Add#4287, ID#4288, Name#4289, SPARK_PARTITION_ID() AS partitionID#4293]\n      +- LogicalRDD [Add#4287, ID#4288, Name#4289], false\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- == Initial Plan ==\n   BroadcastHashJoin [Add#4315], [Add#4287], Inner, BuildRight, false, true\n   :- Filter isnotnull(Add#4315)\n   :  +- Project [Add#4315, ID#4316, Name#4317, SPARK_PARTITION_ID() AS partitionID#4321]\n   :     +- Scan ExistingRDD[Add#4315,ID#4316,Name#4317]\n   +- Exchange SinglePartition, EXECUTOR_BROADCAST, [plan_id=1729]\n      +- Filter isnotnull(Add#4287)\n         +- Project [Add#4287, ID#4288, Name#4289, SPARK_PARTITION_ID() AS partitionID#4293]\n            +- Scan ExistingRDD[Add#4287,ID#4288,Name#4289]\n\n== Photon Explanation ==\nPhoton does not fully support the query because:\n\t\tUnsupported node: Scan ExistingRDD[Add#4315,ID#4316,Name#4317].\n\nReference node:\n\tScan ExistingRDD[Add#4315,ID#4316,Name#4317]\n\nPhoton does not fully support the query because:\n\t\tUnsupported node: Scan ExistingRDD[Add#4287,ID#4288,Name#4289].\n\nReference node:\n\tScan ExistingRDD[Add#4287,ID#4288,Name#4289]\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "f = d.join(broadcast(b),d.Add == b.Add)\n",
    "f.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6601d322-0605-4112-95d5-0cc930c06707",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+-----------+---+----+----+-----------+\n|Add|  ID|Name|partitionID|Add|  ID|Name|partitionID|\n+---+----+----+-----------+---+----+----+-----------+\n|IND|2.48|Tina|          2|IND|2.48|Tina|          2|\n+---+----+----+-----------+---+----+----+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "f.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "350db049-a4e3-4bbb-885f-55a42b69ab33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbd21168-1b1c-408d-85dd-275c6babe0e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n|first_name|     city|\n+----------+---------+\n|    andrea| medellin|\n|   rodolfo| medellin|\n|     abdul|bangalore|\n+----------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "peopleDF = (\n",
    "  (\"andrea\", \"medellin\"),\n",
    "  (\"rodolfo\", \"medellin\"),\n",
    "  (\"abdul\", \"bangalore\")\n",
    ")\n",
    "\n",
    "peopleDF_a = sc.parallelize(peopleDF)\n",
    "peopleDF_b = spark.createDataFrame(peopleDF_a, [\"first_name\", \"city\"])\n",
    "\n",
    "peopleDF_b.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d97ac5f6-cbfc-4a5a-8137-d65773c31e9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+----------+\n|     city| country|population|\n+---------+--------+----------+\n| medellin|colombia|       2.5|\n|bangalore|   india|      12.3|\n+---------+--------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "citiesDF = (\n",
    "  (\"medellin\", \"colombia\", 2.5),\n",
    "  (\"bangalore\", \"india\", 12.3)\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "citiesDF_a = sc.parallelize(citiesDF)\n",
    "citiesDF_b = spark.createDataFrame(citiesDF_a, (\"city\", \"country\", \"population\"))\n",
    "\n",
    "citiesDF_b.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6398792b-2c4c-4334-875b-c132ac8fc01c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\nJoin Inner, (city#4454 = city#4466)\n:- LogicalRDD [first_name#4453, city#4454], false\n+- ResolvedHint (strategy=broadcast)\n   +- LogicalRDD [city#4466, country#4467, population#4468], false\n\n== Analyzed Logical Plan ==\nfirst_name: string, city: string, city: string, country: string, population: double\nJoin Inner, (city#4454 = city#4466)\n:- LogicalRDD [first_name#4453, city#4454], false\n+- ResolvedHint (strategy=broadcast)\n   +- LogicalRDD [city#4466, country#4467, population#4468], false\n\n== Optimized Logical Plan ==\nJoin Inner, (city#4454 = city#4466), rightHint=(strategy=broadcast), joinId=5\n:- Filter isnotnull(city#4454)\n:  +- LogicalRDD [first_name#4453, city#4454], false\n+- Filter isnotnull(city#4466)\n   +- LogicalRDD [city#4466, country#4467, population#4468], false\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- == Initial Plan ==\n   BroadcastHashJoin [city#4454], [city#4466], Inner, BuildRight, false, true\n   :- Filter isnotnull(city#4454)\n   :  +- Scan ExistingRDD[first_name#4453,city#4454]\n   +- Exchange SinglePartition, EXECUTOR_BROADCAST, [plan_id=2001]\n      +- Filter isnotnull(city#4466)\n         +- Scan ExistingRDD[city#4466,country#4467,population#4468]\n\n== Photon Explanation ==\nPhoton does not fully support the query because:\n\t\tUnsupported node: Scan ExistingRDD[first_name#4453,city#4454].\n\nReference node:\n\tScan ExistingRDD[first_name#4453,city#4454]\n\nPhoton does not fully support the query because:\n\t\tUnsupported node: Scan ExistingRDD[city#4466,country#4467,population#4468].\n\nReference node:\n\tScan ExistingRDD[city#4466,country#4467,population#4468]\n\n"
     ]
    }
   ],
   "source": [
    "peopleDF_b_j = peopleDF_b.join(\n",
    "  broadcast(citiesDF_b),\n",
    "  peopleDF_b.city == citiesDF_b.city\n",
    ")\n",
    "\n",
    "peopleDF_b_j.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b20e3f3f-bbb3-474c-96b2-00fc148ea6a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\nJoin Inner, (city#4454 = city#4466)\n:- LogicalRDD [first_name#4453, city#4454], false\n+- LogicalRDD [city#4466, country#4467, population#4468], false\n\n== Analyzed Logical Plan ==\nfirst_name: string, city: string, city: string, country: string, population: double\nJoin Inner, (city#4454 = city#4466)\n:- LogicalRDD [first_name#4453, city#4454], false\n+- LogicalRDD [city#4466, country#4467, population#4468], false\n\n== Optimized Logical Plan ==\nJoin Inner, (city#4454 = city#4466)\n:- Filter isnotnull(city#4454)\n:  +- LogicalRDD [first_name#4453, city#4454], false\n+- Filter isnotnull(city#4466)\n   +- LogicalRDD [city#4466, country#4467, population#4468], false\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- == Initial Plan ==\n   SortMergeJoin [city#4454], [city#4466], Inner\n   :- Sort [city#4454 ASC NULLS FIRST], false, 0\n   :  +- Exchange hashpartitioning(city#4454, 200), ENSURE_REQUIREMENTS, [plan_id=2055]\n   :     +- Filter isnotnull(city#4454)\n   :        +- Scan ExistingRDD[first_name#4453,city#4454]\n   +- Sort [city#4466 ASC NULLS FIRST], false, 0\n      +- Exchange hashpartitioning(city#4466, 200), ENSURE_REQUIREMENTS, [plan_id=2056]\n         +- Filter isnotnull(city#4466)\n            +- Scan ExistingRDD[city#4466,country#4467,population#4468]\n\n== Photon Explanation ==\nPhoton does not fully support the query because:\n\t\tUnsupported node: Scan ExistingRDD[first_name#4453,city#4454].\n\nReference node:\n\tScan ExistingRDD[first_name#4453,city#4454]\n\nPhoton does not fully support the query because:\n\t\tUnsupported node: Scan ExistingRDD[city#4466,country#4467,population#4468].\n\nReference node:\n\tScan ExistingRDD[city#4466,country#4467,population#4468]\n\n"
     ]
    }
   ],
   "source": [
    "peopleDF_b_j = peopleDF_b.join(\n",
    "  citiesDF_b,\n",
    "  peopleDF_b.city == citiesDF_b.city\n",
    ")\n",
    "\n",
    "peopleDF_b_j.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf48aa19-ee14-4ea3-ad0d-52a5aeabfd1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Notice how the parsed, analyzed, and optimized logical plans all contain ResolvedHint isBroadcastable=true because the broadcast() function was used. This hint isn’t included when the broadcast() function isn’t used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4a09476-4f72-4b66-8352-33d86d9c9118",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Automatic Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49f8f52b-0391-41a5-920f-d8e3d5d08fa3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- In many cases, Spark can automatically detect whether to use a broadcast join or not, depending on the size of the data. If Spark can detect that one of the joined DataFrames is small (10 MB by default), Spark will automatically broadcast it for us. The code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1798534-c5d3-4c4b-a663-90acb11844d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n'Join UsingJoin(Inner, [id])\n:- Range (1, 10000, step=1, splits=Some(4))\n+- Range (1, 100000000, step=1, splits=Some(4))\n\n== Analyzed Logical Plan ==\nid: bigint\nProject [id#4511L]\n+- Join Inner, (id#4511L = id#4509L)\n   :- Range (1, 10000, step=1, splits=Some(4))\n   +- Range (1, 100000000, step=1, splits=Some(4))\n\n== Optimized Logical Plan ==\nProject [id#4511L]\n+- Join Inner, (id#4511L = id#4509L)\n   :- Range (1, 10000, step=1, splits=Some(4))\n   +- Range (1, 100000000, step=1, splits=Some(4))\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- == Initial Plan ==\n   ColumnarToRow\n   +- PhotonResultStage\n      +- PhotonProject [id#4511L]\n         +- PhotonBroadcastHashJoin [id#4511L], [id#4509L], Inner, BuildLeft, false, true\n            :- PhotonShuffleExchangeSource\n            :  +- PhotonShuffleMapStage\n            :     +- PhotonShuffleExchangeSink SinglePartition\n            :        +- PhotonRange Range (1, 10000, step=1, splits=4)\n            +- PhotonRange Range (1, 100000000, step=1, splits=4)\n\n== Photon Explanation ==\nThe query is fully supported by Photon.\n"
     ]
    }
   ],
   "source": [
    "bigTable = spark.range(1, 100000000)\n",
    "smallTable = spark.range(1, 10000) # size estimated by Spark - auto-broadcast\n",
    "joinedNumbers = smallTable.join(bigTable, \"id\")\n",
    "joinedNumbers.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f1b1381-bce6-4916-ab1d-3ca6c866a3ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- However, in the previous case, Spark did not detect that the small table could be broadcast. \n",
    "- The reason is that Spark will not determine the size of a local collection because it might be big, and evaluating its size may be an O(N) operation, which can defeat the purpose before any computation is made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b5526bd-9932-437f-b5e2-ce98f093a862",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Spark will perform auto-detection when\n",
    "    - it constructs a DataFrame from scratch, e.g. spark.range\n",
    "    - it reads from files with schema and/or size information, e.g. Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "087be9f6-396f-4b31-a918-03689d5db50e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Configuring Broadcast Join Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "475dad7e-fae3-42fa-a823-32aa629343ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- The threshold for automatic broadcast join detection can be tuned or disabled.\n",
    "- The configuration is spark.sql.autoBroadcastJoinThreshold, and the value is taken in bytes. If you want to configure it to another number, we can set it in the SparkSession:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5ad489a-f5d7-4575-bd52-f5f99fcebcf5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 104857600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eddd19e6-da01-4f71-99ec-cab2f8eef41e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- or deactivate it altogether by setting the value to -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b0ef0ca-875d-4ac8-b206-74566851c564",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d178623-119d-4e28-a239-8f3cf6d12385",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04-Spark-Performance-Tuning-Joins",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}